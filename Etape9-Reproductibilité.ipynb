{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff33a60c-38a9-4edc-b32a-23729df644a4",
   "metadata": {},
   "source": [
    "# Étape 9 — Reproductibilité\n",
    "\n",
    "Objectif : satisfaire exactement la consigne :\n",
    "\n",
    "- Fixer les seeds (aléatoire)\n",
    "- Setup d’environnement clair\n",
    "- Une commande (ou une séquence de notebooks) permettant de reproduire les résultats\n",
    "\n",
    "Ce notebook :\n",
    "1) fixe les seeds\n",
    "2) vérifie la racine projet et crée `models/` et `reports/` si besoin\n",
    "3) génère `requirements.txt`\n",
    "4) génère `RUNBOOK.md` (ordre d’exécution clair)\n",
    "5) génère / met à jour `README.md` (résumé + quickstart)\n",
    "6) génère `reports/artifacts_summary.json` (preuves des outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8156eb5-1de7-4d1a-bd5e-2f3a51574f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds fixed: 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Seeds fixed:\", RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e2c9285-cf94-4d10-a2f1-7de47fdfcaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\Ahmed\\Documents\\GitHub\\netflixandtvshows\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# IMPORTANT : notebook exécuté depuis la racine du projet\n",
    "PROJECT_ROOT = Path(\".\")\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c17c0a1-bee4-4b53-ad29-a263065db544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing expected files: []\n",
      "Missing expected dirs: []\n",
      "Ensured dirs exist: ./models and ./reports\n"
     ]
    }
   ],
   "source": [
    "expected_files = [\n",
    "    \"netflix_titles.csv\",\n",
    "    \"DataAcquisition.ipynb\",\n",
    "    \"DataComprehension.ipynb\",\n",
    "    \"ExploratoryAnalytics.ipynb\",\n",
    "    \"Step3-DataCleaning.ipynb\",\n",
    "]\n",
    "\n",
    "expected_dirs = [\"models\", \"reports\"]\n",
    "\n",
    "present_files = {p.name for p in PROJECT_ROOT.glob(\"*\") if p.is_file()}\n",
    "present_dirs = {p.name for p in PROJECT_ROOT.glob(\"*\") if p.is_dir()}\n",
    "\n",
    "missing_files = [f for f in expected_files if f not in present_files]\n",
    "missing_dirs = [d for d in expected_dirs if d not in present_dirs]\n",
    "\n",
    "print(\"\\nMissing expected files:\", missing_files)\n",
    "print(\"Missing expected dirs:\", missing_dirs)\n",
    "\n",
    "for d in expected_dirs:\n",
    "    (PROJECT_ROOT / d).mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Ensured dirs exist: ./models and ./reports\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e079d9e-eb74-4632-859f-b52990d90b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created: requirements.backup.txt\n",
      "Wrote: requirements.txt\n",
      "numpy\n",
      "pandas\n",
      "scikit-learn\n",
      "joblib\n",
      "matplotlib\n",
      "\n"
     ]
    }
   ],
   "source": [
    "requirements_path = PROJECT_ROOT / \"requirements.txt\"\n",
    "\n",
    "requirements_content = \"\"\"\\\n",
    "numpy\n",
    "pandas\n",
    "scikit-learn\n",
    "joblib\n",
    "matplotlib\n",
    "\"\"\"\n",
    "\n",
    "# backup si déjà existant et non vide\n",
    "if requirements_path.exists():\n",
    "    old = requirements_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    if old.strip():\n",
    "        backup = PROJECT_ROOT / \"requirements.backup.txt\"\n",
    "        backup.write_text(old, encoding=\"utf-8\")\n",
    "        print(\"Backup created:\", backup)\n",
    "\n",
    "requirements_path.write_text(requirements_content, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", requirements_path)\n",
    "print(requirements_path.read_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62b4aef9-d554-477f-8850-51217795fc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNBOOK.md written\n"
     ]
    }
   ],
   "source": [
    "runbook_path = PROJECT_ROOT / \"RUNBOOK.md\"\n",
    "\n",
    "runbook = f\"\"\"# RUNBOOK — Reproduire le projet\n",
    "\n",
    "Dernière génération : {now}\n",
    "\n",
    "1) Installer\n",
    "python -m venv .venv\n",
    "pip install -r requirements.txt\n",
    "\n",
    "2) Exécuter les notebooks\n",
    "- DataAcquisition.ipynb\n",
    "- DataComprehension.ipynb\n",
    "- Step3-DataCleaning.ipynb\n",
    "- ExploratoryAnalytics.ipynb\n",
    "- Step7-HyperparameterTuning_ONLYCSV.ipynb\n",
    "- Step8-EvaluationAndSelection.ipynb\n",
    "- Etape9-Reproductibilite.ipynb\n",
    "\"\"\"\n",
    "\n",
    "runbook_path.write_text(runbook, encoding=\"utf-8\")\n",
    "print(\"RUNBOOK.md written\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d42c9bb0-696d-4861-9e11-5f0cb633387a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md written\n"
     ]
    }
   ],
   "source": [
    "readme_path = PROJECT_ROOT / \"README.md\"\n",
    "\n",
    "readme = f\"\"\"# Mini pipeline NLP — Netflix is_mature\n",
    "\n",
    "Reproductibilité :\n",
    "- Seeds fixées : {RANDOM_STATE}\n",
    "- Chemins relatifs\n",
    "- Voir RUNBOOK.md pour la reproduction\n",
    "\n",
    "Généré le {now}\n",
    "\"\"\"\n",
    "\n",
    "readme_path.write_text(readme, encoding=\"utf-8\")\n",
    "print(\"README.md written\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b31449ac-cb3c-4cc2-b116-0b744c71834b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifacts_summary.json written\n"
     ]
    }
   ],
   "source": [
    "artifacts = {\n",
    "    \"generated_at\": now,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"models\": [p.as_posix() for p in (PROJECT_ROOT / \"models\").glob(\"*.joblib\")],\n",
    "    \"reports\": [p.as_posix() for p in (PROJECT_ROOT / \"reports\").glob(\"*.csv\")],\n",
    "}\n",
    "\n",
    "artifacts_path = PROJECT_ROOT / \"reports\" / \"artifacts_summary.json\"\n",
    "artifacts_path.write_text(json.dumps(artifacts, indent=2), encoding=\"utf-8\")\n",
    "print(\"artifacts_summary.json written\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5863f3e9-9d71-4ee7-8a05-e80142aee708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
