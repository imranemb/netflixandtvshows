{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d835cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook working directory: c:\\Users\\imran\\Desktop\\Nouveau dossier (2)\\netflixandtvshows\\notebooks\n",
      "CSV exists (relative): False\n",
      "CSV absolute path exists: False\n",
      "Error: 'netflix_titles.csv' not found at either path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 1: Data Acquisition\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Check working directory and CSV availability\n",
    "print(\"Notebook working directory:\", os.getcwd())\n",
    "print(\"CSV exists (relative):\", os.path.exists('netflix_titles.csv'))\n",
    "print(\"CSV absolute path exists:\", os.path.exists('/Users/ragnar/Desktop/PROJET/netflix_titles.csv'))\n",
    "\n",
    "# Load the dataset (use relative path if available, otherwise fall back to absolute)\n",
    "try:\n",
    "    csv_path = 'netflix_titles.csv'\n",
    "    if not os.path.exists(csv_path):\n",
    "        csv_path = '/Users/ragnar/Desktop/PROJET/netflix_titles.csv'\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(f\"Dimensions: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'netflix_titles.csv' not found at either path.\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "# Display first rows to understand structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe437ef1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'rating'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# STEP 3: Cleaning & Preprocessing\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# I need to clean the 'rating' column because some values are durations (like '74 min')\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# So I keep only the real ratings\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m valid_ratings = [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrating\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.unique() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(r, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m r]\n\u001b[32m      6\u001b[39m df_clean = df[df[\u001b[33m'\u001b[39m\u001b[33mrating\u001b[39m\u001b[33m'\u001b[39m].isin(valid_ratings)].copy()\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Removing rows where description or rating is missing (NaN)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\imran\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\imran\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'rating'"
     ]
    }
   ],
   "source": [
    "# STEP 3: Cleaning & Preprocessing\n",
    "\n",
    "# I need to clean the 'rating' column because some values are durations (like '74 min')\n",
    "# So I keep only the real ratings\n",
    "valid_ratings = [r for r in df['rating'].unique() if isinstance(r, str) and 'min' not in r]\n",
    "df_clean = df[df['rating'].isin(valid_ratings)].copy()\n",
    "\n",
    "# Removing rows where description or rating is missing (NaN)\n",
    "df_clean.dropna(subset=['rating', 'description'], inplace=True)\n",
    "\n",
    "# Creating the binary target 'is_mature'\n",
    "# If the rating is for adults (TV-MA, R, etc.), it's 1. Otherwise it's 0.\n",
    "mature_labels = ['TV-MA', 'R', 'NC-17', 'UR']\n",
    "df_clean['is_mature'] = df_clean['rating'].apply(lambda x: 1 if x in mature_labels else 0)\n",
    "\n",
    "print(\"Check target balance:\")\n",
    "print(df_clean['is_mature'].value_counts())\n",
    "\n",
    "# Defining X (features) and y (target)\n",
    "X = df_clean['description']\n",
    "y = df_clean['is_mature']\n",
    "\n",
    "# Splitting the dataset into Train and Test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# I use stratify=y to keep the same percentage of mature movies in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d576e0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.8.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.24.1 in /Users/ragnar/Desktop/PROJET/.venv/lib/python3.13/site-packages (from scikit-learn) (2.4.0)\n",
      "Collecting scipy>=1.10.0 (from scikit-learn)\n",
      "  Using cached scipy-1.16.3-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.8.0-cp313-cp313-macosx_12_0_arm64.whl (8.0 MB)\n",
      "Using cached joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Using cached scipy-1.16.3-cp313-cp313-macosx_14_0_arm64.whl (20.9 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.3 scikit-learn-1.8.0 scipy-1.16.3 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f916ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# STEP 5: Model Training & Evaluation (Detailed Version)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# --- 1. RECHARGEMENT DES DONNÉES (Sécurité) ---\n",
    "# On s'assure que tout est prêt même si on relance juste cette cellule\n",
    "try:\n",
    "    df = pd.read_csv('netflix_titles.csv')\n",
    "    valid_ratings = [r for r in df['rating'].unique() if isinstance(r, str) and 'min' not in r]\n",
    "    df_clean = df[df['rating'].isin(valid_ratings)].copy()\n",
    "    df_clean.dropna(subset=['rating', 'description'], inplace=True)\n",
    "    mature_labels = ['TV-MA', 'R', 'NC-17', 'UR']\n",
    "    df_clean['is_mature'] = df_clean['rating'].apply(lambda x: 1 if x in mature_labels else 0)\n",
    "    \n",
    "    X = df_clean['description']\n",
    "    y = df_clean['is_mature']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    print(\"Data Ready.\")\n",
    "except Exception as e:\n",
    "    print(f\"Data loading skipped or failed: {e}\")\n",
    "\n",
    "# --- 2. CONFIGURATION DES MODÈLES ---\n",
    "\n",
    "# Pipeline 1: Logistic Regression\n",
    "pipe_lr = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english', max_features=5000)),\n",
    "    ('clf', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Pipeline 2: Random Forest\n",
    "pipe_rf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english', max_features=5000)),\n",
    "    ('clf', RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Pipeline 3: SVM\n",
    "pipe_svm = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english', max_features=5000)),\n",
    "    ('clf', LinearSVC(random_state=42))\n",
    "])\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': pipe_lr,\n",
    "    'Random Forest': pipe_rf,\n",
    "    'SVM': pipe_svm\n",
    "}\n",
    "\n",
    "# --- 3. ENTRAÎNEMENT ET RÉSULTATS DÉTAILLÉS ---\n",
    "\n",
    "print(\"Starting detailed evaluation...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"================ {name} ================\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # 1. Accuracy\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Global Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # 2. Detailed Report (Precision, Recall, F1)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['General', 'Mature']))\n",
    "    \n",
    "    # 3. Visual Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['General', 'Mature'], \n",
    "                yticklabels=['General', 'Mature'])\n",
    "    plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"All models evaluated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
